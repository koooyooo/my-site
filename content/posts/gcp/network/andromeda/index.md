---
title: "Andromeda"
date: 2021-01-04T02:21:51+09:00
draft: true
---

GoogleのSDNシステムの中でも 個人的に注目している Andromedaに関して少し勉強してみました。

<!-- {{<youtube 9CbcIfZ3zH4>}} -->
<!-- {{< youtube enhjtnHm0CY >}} -->

## 概要
`Andromeda`は`Google`のネットワーク仮想化環境で、VPC, 内部ロードバランサ、NAT、ライブマイグレーション、といったGCP上の各種サービスの実現に大きく寄与しています。発表されたのは2014年で、Googleのネットワーク系システムの中では比較的最近できたものとなります。

ここで、Googleのネットワーク系ソリューションの系譜を見てみたいと思います。2006年の Google Cloud Cache 以降、データセンターの内外に渡り様々なシステムが誕生していることが分かります。個人的にGCPに期待している大きな理由の一つが、この強力なネットワーク基盤です。

| # | 発表 | 名称 | 役割 | 備考 |
| --- | --- | --- | --- | --- |
| 1 | 2006 | Google Cloud Cache | | |
| 2 | 2008 | Watchtower | | |
| 3 | ? | Freedome | | |
| 4 | ? | Maglev | SD ロードバランサー | |
| 5 | 2010 | Onix | | |
| 6 | 2010 | BwE | | |
| 7 | ? | B4 | SD WAN | |
| 8 | 2012 | jupiter | SD データセンター | |
| 9 | 2014 | `Andromeda` | ネットワーク仮想化 | |
| 10 | 2015 | QUIC | | |
| 11 | 2017 | Espresso | SD エッジ | |

&nbsp;
> *1) SD = Software Defined の略として記載

## ゴール
Andromedaのゴールに関しては次のとおりです。

#### 1. 独立した仮想ネットワーク
外部ネットワークと通信可能であり、逆にポリシー次第ではそれらを隔離できること（例えば次の様なネットワーク）
- 内部クラウドサービス
- 3rdパーティのサービス
- インターネット
  
#### 2. 機能の定期追加が可能
機能のコンスタントな追加・更新ができること（例えば次のような機能）
- 課金
- DoS防御
- パフォーマンス監視
- ファイヤーウォール

#### 3. 基盤として充分な可用性
様々なサービスを下支えする通信基盤として以下を提供できること
- 充分な可用性
- 障害の局所化

#### 4. 運用サポートとしてのライブマイグレーション
運用上、次の事項を達成する手段として、ライブマイグレーションを実現可能なこと
- 可用性の担保
- 新機能を提供する際のスピードの源泉

#### 5. GCPの急成長に耐える拡張性
具体的には以下のような要素が必要
- 仮想ネットワーク数・ネットワーク内のVM数の双方で急成長するGCPの拡張に耐えられるコントロールプレーン
- オートスケーリングやフェイルオーバーに支障のない低レイテンシ
- MapReduce等で利用する巨大なネットワークでさえオンデマンドで素早く提供できること


## デザイン
デザイン上で特に重要な点はデータプレーンとコントロールプレーンの階層化です。コントロールプレーンはクラスタ管理のレイヤを階層化してゆき世界規模の階層となるようデザインされています。

データプレーンはパケット処理の経路を複数用意しています。
- VMホスト上の `Fast Path`はデータプレーン上の第一経路で、柔軟性の担保とホスト本来の性能を引き出す設計がなされています。
  - Fast Pathを利用するのは次の処理です。それ以外のフローは `Hoberboard`か `Coprocessor`に転送されます。
    - 高パフォーマンス
    - 低レイテンシが必須
  - この経路はパケット辺り300nsのCPU予算を持ち、この実現のため次の要素はの要素は制限されます。
    - 複雑性
    - 保持するステートの量
- VMホスト上には それぞれのVMに対応したThreadを持つ `Coprocessor`が稼働しています。
  - Coprocessorの担当は次の特徴を持つ処理です。
    - 高いCPU利用率
    - 厳しいレイテンシ目標が無い
- VMホスト外には `Hoverbord`と呼ばれる仮想ルーティングを行う専用のゲートウェイが存在し、上記以外のパケットはそちらに振り分けられます。 

コントロールプレーンは動的な通信パターンからアクティブなフローを選出しそれをVMホストに取り込みます。一方、Hoverbordが担当するのは ロングテールで殆どの時間で不活性なフローです。こうする理由は、多くの場合に通信は少数のVMホスト同士で行われるからです。全ての転送ルールを取り込ませなければ、サーバーのメモリやコントロールプレーンのCPU拡張が最適化できます。

## Control Plane
コントールプレーンは 3層で構成されます。

| # | 名前 | 役割 |
| --- | --- | --- |
| 1 | Cluster Management (CM) Layer | ユーザの要望に応じた資源 (Compute, Storage, Network) の割り当て |
| 2 | Fabric Magnagement (FM) Layer | CM層に向け 高レベルのAPIを提供 (Switch, カプセル化Format, Network要素) |
| 3 | Switch Layer | 2種類のソフトウェアスイッチ実装 (Forwarding, Firewall, LB) |

&nbsp;  
1番目のCM層は Androidに限ったインフラではないため、以降は 2, 3に関して詳細化してゆきます。

### FM Layer
CM接続時に CM情報の全更新が、以降差分更新が通知される部分。主だった管理情報は以下の通り。

| # | 分類 | 情報 |
| --- | --- | --- |
| 1 | N/W | QoS, Firewall設定 |
| 2 | VM | 外部IP, 内部IP, タグ |
| 3 | Subnet | IP接頭語 | 
| 4 | Route | IP接頭語, 優先順, 次回Hop |

&nbsp;  
`VM Controller (VMC)` は OpenFlowと独自拡張を切り替えながら利用し、VMホストの操作を行います。OpenFlow利用時には RPC経由で `OpenFlow Front End (OFE)`にリクエストを送信します。OFEは受信したリクエストをOpenFlowプロトコルに変換します。OFEはスイッチのイベントをVMCに通知します。

{{< alert type="info" >}}
  Hello
{{< /alert >}}

## 参考
- [Paper] https://www.usenix.org/system/files/conference/nsdi18/nsdi18-dalton.pdf
